{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training an FNO with incremental meta-learning\nA demo of the Incremental FNO meta-learning algorithm on our small Darcy-Flow dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nimport sys\nfrom neuralop.models import FNO\nfrom neuralop.data.datasets import load_darcy_flow_small\nfrom neuralop.utils import count_model_params\nfrom neuralop.training import AdamW\nfrom neuralop.training.incremental import IncrementalFNOTrainer\nfrom neuralop.data.transforms.data_processors import IncrementalDataProcessor\nfrom neuralop import LpLoss, H1Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the Darcy flow dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader, test_loaders, output_encoder = load_darcy_flow_small(\n    n_train=100,\n    batch_size=16,\n    test_resolutions=[16, 32],\n    n_tests=[100, 50],\n    test_batch_sizes=[32, 32],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose device\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the incremental FNO model\nWe start with 2 modes in each dimension\nWe choose to update the modes by the incremental gradient explained algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "incremental = True\nif incremental:\n    starting_modes = (2, 2)\nelse:\n    starting_modes = (16, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "set up model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = FNO(\n    max_n_modes=(16, 16),\n    n_modes=starting_modes,\n    hidden_channels=32,\n    in_channels=1,\n    out_channels=1,\n)\nmodel = model.to(device)\nn_params = count_model_params(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the optimizer and scheduler\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n\n\n# If one wants to use Incremental Resolution, one should use the IncrementalDataProcessor - When passed to the trainer, the trainer will automatically update the resolution\n# Incremental_resolution : bool, default is False\n#    if True, increase the resolution of the input incrementally\n#    uses the incremental_res_gap parameter\n#    uses the subsampling_rates parameter - a list of resolutions to use\n#    uses the dataset_indices parameter - a list of indices of the dataset to slice to regularize the input resolution\n#    uses the dataset_resolution parameter - the resolution of the input\n#    uses the epoch_gap parameter - the number of epochs to wait before increasing the resolution\n#    uses the verbose parameter - if True, print the resolution and the number of modes\ndata_transform = IncrementalDataProcessor(\n    in_normalizer=None,\n    out_normalizer=None,\n    device=device,\n    subsampling_rates=[2, 1],\n    dataset_resolution=16,\n    dataset_indices=[2, 3],\n    epoch_gap=10,\n    verbose=True,\n)\n\ndata_transform = data_transform.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the losses\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2loss = LpLoss(d=2, p=2)\nh1loss = H1Loss(d=2)\ntrain_loss = h1loss\neval_losses = {\"h1\": h1loss, \"l2\": l2loss}\nprint(\"\\n### N PARAMS ###\\n\", n_params)\nprint(\"\\n### OPTIMIZER ###\\n\", optimizer)\nprint(\"\\n### SCHEDULER ###\\n\", scheduler)\nprint(\"\\n### LOSSES ###\")\nprint(\"\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\")\nprint(f\"\\n * Train: {train_loss}\")\nprint(f\"\\n * Test: {eval_losses}\")\nsys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the IncrementalTrainer\nother options include setting incremental_loss_gap = True\nIf one wants to use incremental resolution set it to True\nIn this example we only update the modes and not the resolution\nWhen using the incremental resolution one should keep in mind that the numnber of modes initially set should be strictly less than the resolution\nAgain these are the various paramaters for the various incremental settings\nincremental_grad : bool, default is False\n   if True, use the base incremental algorithm which is based on gradient variance\n   uses the incremental_grad_eps parameter - set the threshold for gradient variance\n   uses the incremental_buffer paramater - sets the number of buffer modes to calculate the gradient variance\n   uses the incremental_max_iter parameter - sets the initial number of iterations\n   uses the incremental_grad_max_iter parameter - sets the maximum number of iterations to accumulate the gradients\nincremental_loss_gap : bool, default is False\n   if True, use the incremental algorithm based on loss gap\n   uses the incremental_loss_eps parameter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Finally pass all of these to the Trainer\ntrainer = IncrementalFNOTrainer(\n    model=model,\n    n_epochs=20,\n    data_processor=data_transform,\n    device=device,\n    verbose=True,\n    incremental_loss_gap=False,\n    incremental_grad=True,\n    incremental_grad_eps=0.9999,\n    incremental_loss_eps = 0.001,\n    incremental_buffer=5,\n    incremental_max_iter=1,\n    incremental_grad_max_iter=2,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.train(\n    train_loader,\n    test_loaders,\n    optimizer,\n    scheduler,\n    regularizer=False,\n    training_loss=train_loss,\n    eval_losses=eval_losses,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the prediction, and compare with the ground-truth\nNote that we trained on a very small resolution for\na very small number of epochs\nIn practice, we would train at larger resolution, on many more samples.\n\nHowever, for practicity, we created a minimal example that\ni) fits in just a few Mb of memory\nii) can be trained quickly on CPU\n\nIn practice we would train a Neural Operator on one or multiple GPUs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_samples = test_loaders[32].dataset\n\nfig = plt.figure(figsize=(7, 7))\nfor index in range(3):\n    data = test_samples[index]\n    # Input x\n    x = data[\"x\"].to(device)\n    # Ground-truth\n    y = data[\"y\"].to(device)\n    # Model prediction\n    out = model(x.unsqueeze(0))\n    ax = fig.add_subplot(3, 3, index * 3 + 1)\n    x = x.cpu().squeeze().detach().numpy()\n    y = y.cpu().squeeze().detach().numpy()\n    ax.imshow(x, cmap=\"gray\")\n    if index == 0:\n        ax.set_title(\"Input x\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    ax = fig.add_subplot(3, 3, index * 3 + 2)\n    ax.imshow(y.squeeze())\n    if index == 0:\n        ax.set_title(\"Ground-truth y\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    ax = fig.add_subplot(3, 3, index * 3 + 3)\n    ax.imshow(out.cpu().squeeze().detach().numpy())\n    if index == 0:\n        ax.set_title(\"Model prediction\")\n    plt.xticks([], [])\n    plt.yticks([], [])\n\nfig.suptitle(\"Inputs, ground-truth output and prediction.\", y=0.98)\nplt.tight_layout()\nfig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}